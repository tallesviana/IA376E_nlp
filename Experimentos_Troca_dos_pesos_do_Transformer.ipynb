{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia4NaY_f_LAX"
      },
      "source": [
        "**Pré-treino em voz auxilia em tarefas de texto?**\n",
        "\n",
        "Esse notebook destina-se à verificar se a utilização dos pesos do Wa2Vec2 pré-treinado em voz auxiliar o BERT em tarefas de texto *downstream*.\n",
        "\n",
        "\n",
        "\n",
        "*   Os testes são feitos sempre utilizando a arquitetura do BERT, no qual faz-se o estudo apenas da troca dos pesos do Transformer;\n",
        "*   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DlF6OTwTK7X"
      },
      "source": [
        "---\n",
        "### Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9op7IIX5qB9y"
      },
      "outputs": [],
      "source": [
        "# ! pip -q install --upgrade requests  # Dependendo da runtime do COLAB, um upgrade dessas libs pode ser necessário\n",
        "# ! pip -q install --upgrade urllib3\n",
        "! pip -q install lightning-bolts\n",
        "! pip -q install pytorch_lightning\n",
        "! pip -q install transformers\n",
        "! pip -q install neptune-client\n",
        "! pip -q install adabelief-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gObVI__zrHa_"
      },
      "source": [
        "IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYqKp8L8rFm4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import NeptuneLogger\n",
        "\n",
        "from transformers import BertTokenizer, Wav2Vec2CTCTokenizer\n",
        "from transformers import BertModel, BertForSequenceClassification, Wav2Vec2Model\n",
        "from transformers import BertConfig\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "from adabelief_pytorch import AdaBelief\n",
        "\n",
        "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
        "\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-m0eD398Sz_"
      },
      "source": [
        "---\n",
        "# IMDB - DATASET\n",
        "\n",
        "A priori, o dataset do IMDB é escolhido por ser pequeno e podermos avaliar rapidamente a performance dos modelos em testes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAK9jQlv8i0T"
      },
      "outputs": [],
      "source": [
        "!wget -q -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO0LIg0w8waU"
      },
      "outputs": [],
      "source": [
        "# Seed para reprodutibilidade\n",
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "max_valid = 5000\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "x_valid = x_train[-max_valid:]\n",
        "y_valid = y_train[-max_valid:]\n",
        "x_train = x_train[:-max_valid]\n",
        "y_train = y_train[:-max_valid]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y8WGai_DoIH"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset():\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], int(self.y[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol21QvD4Dp9e"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(x, y, tokenizer, batch_size, shuffle=False, max_length=250):\n",
        "  def data_collator(batch):\n",
        "    x, y = zip(*batch)\n",
        "    tokenized_x = tokenizer(x, padding='longest', truncation=True, max_length=max_length, return_tensors='pt')\n",
        "    return tokenized_x['input_ids'], tokenized_x['attention_mask'], torch.LongTensor(y)\n",
        "  dataset = IMDBDataset(x, y)\n",
        "  return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U5VOwaHs4zG"
      },
      "source": [
        "# OS MODELOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3f5CyWRDXk5"
      },
      "source": [
        "### Definição do tokenizador\n",
        "- Nesse caso, para o BERT base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzhlmTfh9TPK"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUG0Hxt9BjP5"
      },
      "source": [
        "### Mapeamento de pesos\n",
        "\n",
        "Primeiramente, para utilizar os pesos do Wa2Vec2 no BERT precisamos realizar um mapeamento dos nomes dos parâmetros, e uma função para alteração dos mesmo no *state_dict*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZRQMk9eBiEz"
      },
      "outputs": [],
      "source": [
        "MAP_WAV2VEC_TO_BERT_NAMES = {\n",
        "    'attention.k_proj.weight': 'attention.self.key.weight',\n",
        "    'attention.k_proj.bias': 'attention.self.key.bias',\n",
        "    'attention.v_proj.weight': 'attention.self.value.weight',\n",
        "    'attention.v_proj.bias': 'attention.self.value.bias',\n",
        "    'attention.q_proj.weight': 'attention.self.query.weight',\n",
        "    'attention.q_proj.bias': 'attention.self.query.bias',\n",
        "    'attention.out_proj.weight': 'attention.output.dense.weight',\n",
        "    'attention.out_proj.bias': 'attention.output.dense.bias',\n",
        "    'layer_norm.weight': 'attention.output.LayerNorm.weight',\n",
        "    'layer_norm.bias': 'attention.output.LayerNorm.bias',\n",
        "    'feed_forward.intermediate_dense.weight': 'intermediate.dense.weight',\n",
        "    'feed_forward.intermediate_dense.bias': 'intermediate.dense.bias',\n",
        "    'feed_forward.output_dense.weight': 'output.dense.weight',\n",
        "    'feed_forward.output_dense.bias': 'output.dense.bias',\n",
        "    'final_layer_norm.weight': 'output.LayerNorm.weight',\n",
        "    'final_layer_norm.bias': 'output.LayerNorm.bias',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YP325po1T_z"
      },
      "outputs": [],
      "source": [
        "def map_model_state_from_w2v2_to_bert(w2v2_states, bert_states):\n",
        "    print(\"Changing weights ...\")\n",
        "    BERT_PREFIX = 'encoder.layer'\n",
        "    new_weights = {}\n",
        "    for name, weight in w2v2_states.items():\n",
        "        if 'encoder.layers.' in name:\n",
        "            pieces = name.split('.')\n",
        "            head_number = pieces[2]\n",
        "            in_name = '.'.join(pieces[3:])\n",
        "            eq_name = f'{BERT_PREFIX}.{head_number}.{MAP_WAV2VEC_TO_BERT_NAMES[in_name]}'\n",
        "            print(\"Updating: \", eq_name, eq_name in bert_states.keys())\n",
        "            new_weights[eq_name] = weight\n",
        "\n",
        "    bert_states.update(new_weights)\n",
        "    return bert_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kptn39TEF1X"
      },
      "source": [
        "## Lista de modelos para teste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng1DK6lzTRsS"
      },
      "source": [
        "### .1. BERT - modelo usado como base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwz3GNTkxrSn"
      },
      "outputs": [],
      "source": [
        "class BERTBaseUncased(nn.Module):\n",
        "    def __init__(self, num_class, bert_model):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = BertModel.from_pretrained(bert_model)\n",
        "        self.dropout = nn.Dropout(p=0.1, inplace=False)\n",
        "        self.classification = nn.Linear(self.bert.config.hidden_size, num_class)\n",
        "\n",
        "    def freeze_encoder(self, exclude_layernorm=False):\n",
        "        for name, param in self.bert.encoder.named_parameters():\n",
        "            if not exclude_layernorm or 'LayerNorm' not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def unfreeze_encoder(self, exclude_layernorm=False):\n",
        "        for name, param in self.bert.encoder.named_parameters():\n",
        "            if not exclude_layernorm or 'LayerNorm' not in name:\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def forward(self, tokens, masks):\n",
        "        out_bert = self.bert(tokens, attention_mask = masks).pooler_output\n",
        "        out_bert = self.dropout(out_bert)\n",
        "        logits = self.classification(out_bert)\n",
        "        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b7cf6ScTZ-Z"
      },
      "source": [
        "### .2. BERT - base + pesos do Transformer do Wav2Vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hSJiIfU1P1C"
      },
      "outputs": [],
      "source": [
        "class BERTWithWav2Vec2Weights(BERTBaseUncased):\n",
        "    def __init__(self, num_class, bert_model, wav2vec2_states):\n",
        "        super().__init__(num_class, bert_model)\n",
        "        self.load_transformer(wav2vec2_states)\n",
        "\n",
        "    def load_transformer(self, wav2vec2_states):\n",
        "        bert_states = self.bert.state_dict()\n",
        "        bert_states = map_model_state_from_w2v2_to_bert(wav2vec2_states, bert_states)\n",
        "        self.bert.load_state_dict(bert_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io4wQicsh3Mk"
      },
      "source": [
        "### .3. BERT - base + pesos do Transformer inicializados com Xavier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB5pBJsnh1ba"
      },
      "outputs": [],
      "source": [
        "class BERTBaseUncasedXavierInit(BERTBaseUncased):\n",
        "    def __init__(self, num_class, bert_model):\n",
        "        super().__init__(num_class, bert_model)\n",
        "        self.xavier_init_transformer()\n",
        "\n",
        "    def xavier_init_transformer(self):\n",
        "        for i, child in enumerate(self.bert.encoder.layer.children()):\n",
        "            print(\"Xavier Init on HEAD\", i)\n",
        "            torch.nn.init.xavier_uniform_(child.attention.self.query.weight)\n",
        "            child.attention.self.query.bias.data.fill_(0.01)\n",
        "\n",
        "            torch.nn.init.xavier_uniform_(child.attention.self.key.weight)\n",
        "            child.attention.self.key.bias.data.fill_(0.01)\n",
        "\n",
        "            torch.nn.init.xavier_uniform_(child.attention.self.value.weight)\n",
        "            child.attention.self.value.bias.data.fill_(0.01)\n",
        "\n",
        "            torch.nn.init.xavier_uniform_(child.attention.output.dense.weight)\n",
        "            child.attention.output.dense.bias.data.fill_(0.01)\n",
        "\n",
        "            child.attention.output.LayerNorm.reset_parameters()\n",
        "\n",
        "            torch.nn.init.xavier_uniform_(child.intermediate.dense.weight)\n",
        "            child.intermediate.dense.bias.data.fill_(0.01)\n",
        "            \n",
        "            torch.nn.init.xavier_uniform_(child.output.dense.weight)\n",
        "            child.output.dense.bias.data.fill_(0.01)\n",
        "\n",
        "            child.output.LayerNorm.reset_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRO_i8as4-Jg"
      },
      "source": [
        "### .4. BERT - puro + pesos do Transformer do Wav2Vec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPAWnjEn4oSN"
      },
      "outputs": [],
      "source": [
        "class BERTRawWithWav2Vec2Weights(nn.Module):\n",
        "    def __init__(self, num_class, wav2vec2_states):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bert = BertModel(BertConfig())\n",
        "        self.dropout = nn.Dropout(p=0.1, inplace=False)\n",
        "        self.classification = nn.Linear(self.bert.config.hidden_size, num_class)\n",
        "\n",
        "        self.load_transformer(wav2vec2_states)\n",
        "    \n",
        "    def load_transformer(self, wav2vec2_states):\n",
        "        bert_states = self.bert.state_dict()\n",
        "        bert_states = map_model_state_from_w2v2_to_bert(wav2vec2_states, bert_states)\n",
        "        self.bert.load_state_dict(bert_states)\n",
        "\n",
        "    def freeze_encoder(self, exclude_layernorm=False):\n",
        "        for name, param in self.bert.encoder.named_parameters():\n",
        "            if not exclude_layernorm or 'LayerNorm' not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def unfreeze_encoder(self, exclude_layernorm=False):\n",
        "        for name, param in self.bert.encoder.named_parameters():\n",
        "            if not exclude_layernorm or 'LayerNorm' not in name:\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def forward(self, tokens, masks):\n",
        "        out_bert = self.bert(tokens, attention_mask = masks).pooler_output\n",
        "        out_bert = self.dropout(out_bert)\n",
        "        logits = self.classification(out_bert)\n",
        "        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGAyHGmcH71D"
      },
      "source": [
        "### .5. BERT - puro + pesos do Transformer inicializados com Xavier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuE2488fH5Xd"
      },
      "outputs": [],
      "source": [
        "class BERTRawXavierInit(nn.Module):\n",
        "    def __init__(self, num_class):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel(BertConfig())\n",
        "        self.dropout = nn.Dropout(p=0.1, inplace=False)\n",
        "        self.classification = nn.Linear(self.bert.config.hidden_size, num_class)\n",
        "\n",
        "        self.xavier_init_transformer()\n",
        "\n",
        "    def xavier_init_transformer(self):\n",
        "        for i, child in enumerate(self.bert.encoder.layer.children()):\n",
        "            print(\"Xavier Init on HEAD\", i)\n",
        "            torch.nn.init.xavier_uniform_(child.attention.self.query.weight)\n",
        "            child.attention.self.query.bias.data.fill_(0.01)\n",
        "\n",
        "            torch.nn.init.xavier_uniform_(child.attention.self.key.weight)\n",
        "            child.attention.self.key.bias.data.fill_(0.01)\n",
        "\n",
        "            torch.nn.init.xavier_uniform_(child.attention.self.value.weight)\n",
        "            child.attention.self.value.bias.data.fill_(0.01)\n",
        "\n",
        "            torch.nn.init.xavier_uniform_(child.attention.output.dense.weight)\n",
        "            child.attention.output.dense.bias.data.fill_(0.01)\n",
        "\n",
        "            child.attention.output.LayerNorm.reset_parameters()\n",
        "\n",
        "            torch.nn.init.xavier_uniform_(child.intermediate.dense.weight)\n",
        "            child.intermediate.dense.bias.data.fill_(0.01)\n",
        "            \n",
        "            torch.nn.init.xavier_uniform_(child.output.dense.weight)\n",
        "            child.output.dense.bias.data.fill_(0.01)\n",
        "\n",
        "            child.output.LayerNorm.reset_parameters()\n",
        "\n",
        "    def freeze_encoder(self, exclude_layernorm=False):\n",
        "        for name, param in self.bert.encoder.named_parameters():\n",
        "            if not exclude_layernorm or 'LayerNorm' not in name:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def unfreeze_encoder(self, exclude_layernorm=False):\n",
        "        for name, param in self.bert.encoder.named_parameters():\n",
        "            if not exclude_layernorm or 'LayerNorm' not in name:\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def forward(self, tokens, masks):\n",
        "        out_bert = self.bert(tokens, attention_mask = masks).pooler_output\n",
        "        out_bert = self.dropout(out_bert)\n",
        "        logits = self.classification(out_bert)\n",
        "        \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg-FEKlwhyJ2"
      },
      "source": [
        "## Pytorch Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toGVLwJqrGUP"
      },
      "outputs": [],
      "source": [
        "class LiteNet(pl.LightningModule):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.hparams.update(hparams)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.model = args[0]['model'](**args[0]['model_args'])\n",
        "        \n",
        "        self.freeze_finetune_updates = hparams[\"freeze_finetune_updates\"]\n",
        "\n",
        "        self.trainloader = args[0]['train_loader']\n",
        "\n",
        "        self.frozen = False\n",
        "        if self.freeze_finetune_updates > 0:\n",
        "            print(\"Freezing model ...\")\n",
        "            self.model.freeze_encoder()\n",
        "            self.frozen = True\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self.trainloader\n",
        "    \n",
        "    def setup(self, stage):\n",
        "        if stage == 'fit':\n",
        "            train_batches = len(self.train_dataloader())\n",
        "            self.train_steps = (self.hparams.max_epochs * train_batches) // self.hparams.accum_grads\n",
        "\n",
        "    def forward(self, tokens, mask):\n",
        "        return self.model(tokens, mask)\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        if self.frozen and self.freeze_finetune_updates < self.global_step:\n",
        "            print(\"UNFREEZING!!\")\n",
        "            self.frozen = False\n",
        "            self.model.unfreeze_encoder()\n",
        "\n",
        "        tokens, mask, y = train_batch\n",
        "\n",
        "        logits = self.forward(tokens, mask)\n",
        "\n",
        "        loss = self.criterion(logits, y)\n",
        "\n",
        "        self.log('loss_step', loss, on_step=True, prog_bar=True)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        loss = torch.stack([x['loss'] for x in outputs]).mean()       \n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "  \n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        \n",
        "        tokens, mask, y = val_batch\n",
        "\n",
        "        logits = self.forward(tokens, mask)\n",
        "\n",
        "        # LOSS\n",
        "        loss = self.criterion(logits, y)\n",
        "\n",
        "        # ACC\n",
        "        preds = logits.argmax(dim=1)\n",
        "        corrects = (preds == y)\n",
        "\n",
        "        return {\"corrects\": corrects, \"val_loss_step\": loss}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        acc_mean = torch.cat([x[\"corrects\"] for x in outputs], dim=0)\n",
        "        acc_mean = acc_mean.sum() / len(acc_mean)\n",
        "        avg_loss = torch.stack([x[\"val_loss_step\"] for x in outputs]).mean()\n",
        "        \n",
        "        self.log(\"val_acc\", acc_mean, prog_bar=True)\n",
        "        self.log(\"val_loss\", avg_loss, prog_bar=True)\n",
        "  \n",
        "    def test_step(self, test_batch, batch_idx):\n",
        "        \n",
        "        tokens, mask, truth = test_batch\n",
        "\n",
        "        out = self.forward(tokens, mask)\n",
        "        preds = torch.argmax(out, dim=1)\n",
        "\n",
        "        corrects = (preds == truth)\n",
        "\n",
        "        return {\"corrects\": corrects}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        acc_mean = torch.cat([x[\"corrects\"] for x in outputs], dim=0)\n",
        "        acc_mean = acc_mean.sum() / len(acc_mean)\n",
        "\n",
        "        self.log(\"test_acc\", acc_mean, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = AdaBelief(self.parameters(),\n",
        "                              lr=self.hparams['lr'],\n",
        "                              eps=1e-16,\n",
        "                              weight_decay=self.hparams[\"w_decay\"])\n",
        "        \n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=self.hparams['warm_up_steps'],\n",
        "            num_training_steps=self.train_steps\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': {\n",
        "                'scheduler': scheduler,\n",
        "                'interval': 'step',\n",
        "                'frequency': 1,\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv3VAm4OCdCL"
      },
      "source": [
        "## Configuração dos parâmetros - hparams"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **model_name**: nome do checkpoint que será salvo;\n",
        "- **bert_model**: modelo pré-treinado do BERT que será usado;\n",
        "- **w2v2_model**: modelo pré-treinado do Wav2Vec2 nos quais os pesos serão utilizados;\n",
        "- **max_length**: tamanho máximo de tokens;\n",
        "- **nb_classes**: no caso da classificação, temos  2 classes;\n",
        "- **lr**: learning-rate após o período de warm-up;\n",
        "- **w_decay**: weight decay utilizado pelo otimizador (AdaBelief, no caso);\n",
        "- **bs**: batch-size\n",
        "- **accum_grads**: quantos acúmulos de gradientes até fazer update dos pesos;\n",
        "- **patience**: paciência do *early-stop*;\n",
        "- **max_epochs**: quantidade de épocas do treinamento;\n",
        "- **freeze_finetune_updates**: quantidade de épocas no qual o Transformer fica congelado;\n",
        "- **warm_up_stes**: quantidade de steps de warm-up da learning-rate;\n",
        "- **seed_value**: seed à ser utilizada;"
      ],
      "metadata": {
        "id": "2KDO6U3tNPrW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRlgm23kCZ3a",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "model_name = \"FINAL_bert_base\" #@param {type: \"string\"}\n",
        "bert_model = 'bert-base-uncased'#@param {type: \"string\"}\n",
        "w2v2_model = \"facebook/wav2vec2-base\"#@param {type: \"string\"}\n",
        "max_length =  256#@param {type: \"integer\"}\n",
        "nb_classes = 2 #@param {type: \"integer\"}\n",
        "lr = 5e-4 #@param {type: \"number\"}\n",
        "w_decay =  1e-4#@param {type: \"number\"}\n",
        "bs =  16#@param {type: \"integer\"}\n",
        "accum_grads =  4#@param {type: \"integer\"}\n",
        "patience =  3#@param {type: \"integer\"}\n",
        "max_epochs =  6#@param {type: \"integer\"}\n",
        "\n",
        "freeze_finetune_updates = 0#@param {type: \"integer\"}\n",
        "warm_up_steps = 0#@param {type: \"integer\"}\n",
        "\n",
        "# clip_value = 0 #@param {type: \"number\"}\n",
        "seed_value = 123#@param {type: \"integer\"}\n",
        "# Define hyperparameters\n",
        "hparams = {\"model_name\": model_name,\n",
        "          \"bert_model\": bert_model,\n",
        "          \"w2v2_model\": w2v2_model,\n",
        "           \"max_length\": max_length,\n",
        "           \"nb_classes\": nb_classes,\n",
        "           \"lr\": lr,\n",
        "           \"w_decay\": w_decay,\n",
        "          \"bs\": bs,\n",
        "          \"patience\": patience,\n",
        "          \"accum_grads\": accum_grads,\n",
        "          \"freeze_finetune_updates\":freeze_finetune_updates,\n",
        "           \"clip_value\": clip_value,\n",
        "          \"max_epochs\": max_epochs,\n",
        "           \"seed_value\": seed_value,\n",
        "           \"warm_up_steps\": warm_up_steps}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6U3S2DuW8fo"
      },
      "source": [
        "# Checagem: Overfit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primeiro, escolha o modelo, descomentando o modelo necessário abaixo:"
      ],
      "metadata": {
        "id": "b6dR6ILuKcwP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jYgWSTYGmNL"
      },
      "outputs": [],
      "source": [
        "#####========== 1. BERT base - BASELINE\n",
        "hparams.update(\n",
        "    {'model': BERTBaseUncased, 'model_args': {\n",
        "        'num_class': hparams['nb_classes'],\n",
        "        'bert_model': hparams['bert_model']}}\n",
        ")\n",
        "\n",
        "#####========== 2. BERT base + Wav2Vec2\n",
        "# w2v2_states = Wav2Vec2Model.from_pretrained(hparams['w2v2_model']).state_dict()\n",
        "# hparams.update(\n",
        "#     {'model': BERTWithWav2Vec2Weights, 'model_args': {\n",
        "#         'num_class': hparams['nb_classes'],\n",
        "#         'bert_model': hparams['bert_model'],\n",
        "#         'wav2vec2_states': w2v2_states}}\n",
        "# )\n",
        "\n",
        "#####========== 3. BERT base + Xavier init\n",
        "# hparams.update(\n",
        "#     {'model': BERTBaseUncasedXavierInit, 'model_args': {\n",
        "#         'num_class': hparams['nb_classes'],\n",
        "#         'bert_model': hparams['bert_model']}}\n",
        "# )\n",
        "\n",
        "#####========== 4. BERT puro + Wav2Vec2\n",
        "# w2v2_states = Wav2Vec2Model.from_pretrained(hparams['w2v2_model']).state_dict()\n",
        "# hparams.update(\n",
        "#     {'model': BERTRawWithWav2Vec2Weights, 'model_args': {\n",
        "#         'num_class': hparams['nb_classes'],\n",
        "#         'wav2vec2_states': w2v2_states}}\n",
        "# )\n",
        "\n",
        "#####========== 5. BERT puro + Xavier init\n",
        "# hparams.update(\n",
        "#     {'model': BERTRawXavierInit, 'model_args': {\n",
        "#         'num_class': hparams['nb_classes']}}\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = create_dataloader(x_train, y_train, tokenizer, hparams['bs'], shuffle=True, max_length=hparams['max_length'])\n",
        "valid_loader = create_dataloader(x_valid, y_valid, tokenizer, hparams['bs'], shuffle=False, max_length=hparams['max_length'])\n",
        "\n",
        "hparams.update(\n",
        "    {'train_loader': train_loader}\n",
        ")\n",
        "\n",
        "model = LiteNet(hparams)\n",
        "\n",
        "trainer = pl.Trainer(gpus=1,\n",
        "                     precision=16,\n",
        "                     max_epochs=50,\n",
        "                     accumulate_grad_batches=hparams[\"accum_grads\"],\n",
        "                     check_val_every_n_epoch=1,\n",
        "                     checkpoint_callback=False, # Disable checkpoint saving.\n",
        "                     overfit_batches=2)\n",
        "\n",
        "trainer.fit(model, train_loader, valid_loader)\n",
        "\n",
        "\n",
        "del model, trainer # Para não ter estouro de mémoria da GPU\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JhGX6tTTKhZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Glq6KFhoW-3T"
      },
      "source": [
        "# Treinamento + Neptune logging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caso desejar salvar o checkpoint do modelo no Google Drive:"
      ],
      "metadata": {
        "id": "JKcTO4O7DBlt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDSli4Q7XBDp"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "checkpoint_dir = 'drive/MyDrive/Cursos/UNICAMP - IA376E - 2S2021/Projeto Final/checkpoints/pl'\n",
        "assert os.path.exists(checkpoint_dir), \"Pasta ainda não existe, por favor criar!\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credenciais do Neptune para log:"
      ],
      "metadata": {
        "id": "i2gJApTuDb4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nep_api_key = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2OTc4Mjk2NS0zNTI1LTRlYTItOWVmMC0yZjc0MDE4ODY5NzYifQ==\"\n",
        "nep_proj = \"otalviana/IA376E-Projeto\"\n",
        "tags = [\"FINAL\", \"IMDB\", \"BERT\"]"
      ],
      "metadata": {
        "id": "B3hmV2fvDfJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udMLbQfc0A5w"
      },
      "outputs": [],
      "source": [
        "pl.seed_everything(hparams['seed_value'], workers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecione o modelo desejado:"
      ],
      "metadata": {
        "id": "DALDdOm6KsYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####========== 1. BERT base - BASELINE\n",
        "hparams.update(\n",
        "    {'model': BERTBaseUncased, 'model_args': {\n",
        "        'num_class': hparams['nb_classes'],\n",
        "        'bert_model': hparams['bert_model']}}\n",
        ")\n",
        "\n",
        "#####========== 2. BERT base + Wav2Vec2\n",
        "# w2v2_states = Wav2Vec2Model.from_pretrained(hparams['w2v2_model']).state_dict()\n",
        "# hparams.update(\n",
        "#     {'model': BERTWithWav2Vec2Weights, 'model_args': {\n",
        "#         'num_class': hparams['nb_classes'],\n",
        "#         'bert_model': hparams['bert_model'],\n",
        "#         'wav2vec2_states': w2v2_states}}\n",
        "# )\n",
        "\n",
        "#####========== 3. BERT base + Xavier init\n",
        "# hparams.update(\n",
        "#     {'model': BERTBaseUncasedXavierInit, 'model_args': {\n",
        "#         'num_class': hparams['nb_classes'],\n",
        "#         'bert_model': hparams['bert_model']}}\n",
        "# )\n",
        "\n",
        "#####========== 4. BERT puro + Wav2Vec2\n",
        "# w2v2_states = Wav2Vec2Model.from_pretrained(hparams['w2v2_model']).state_dict()\n",
        "# hparams.update(\n",
        "#     {'model': BERTRawWithWav2Vec2Weights, 'model_args': {\n",
        "#         'num_class': hparams['nb_classes'],\n",
        "#         'wav2vec2_states': w2v2_states}}\n",
        "# )\n",
        "\n",
        "#####========== 5. BERT puro + Xavier init\n",
        "# hparams.update(\n",
        "#     {'model': BERTRawXavierInit, 'model_args': {\n",
        "#         'num_class': hparams['nb_classes']}}\n",
        "# )"
      ],
      "metadata": {
        "id": "p-8CbbI2KsJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinamento:"
      ],
      "metadata": {
        "id": "vg6ts3WUKxOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = create_dataloader(x_train, y_train, tokenizer, hparams['bs'], shuffle=True, max_length=hparams['max_length'])\n",
        "valid_loader = create_dataloader(x_valid, y_valid, tokenizer, hparams['bs'], shuffle=False, max_length=hparams['max_length'])\n",
        "test_loader = create_dataloader(x_test, y_test, tokenizer, hparams['bs'], shuffle=False, max_length=hparams['max_length'])\n",
        "\n",
        "hparams.update(\n",
        "    {'train_loader': train_loader}\n",
        ")\n",
        "\n",
        "model = LiteNet(hparams)\n",
        "\n",
        "neptune_logger = NeptuneLogger(\n",
        "    api_key=nep_api_key,\n",
        "    project=nep_proj,\n",
        "    tags=tags,\n",
        "    log_model_checkpoints=False\n",
        ")\n",
        "\n",
        "neptune_logger.log_hyperparams(params=hparams)\n",
        "\n",
        "\n",
        "# PL Callbacks\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(filename=hparams['model_name'] + \"-{epoch:02d}-{val_acc:.2f}\",\n",
        "                                                  dirpath=checkpoint_dir,\n",
        "                                                  save_top_k=1,\n",
        "                                                  verbose = True, \n",
        "                                                  monitor=\"val_loss\", mode=\"min\")\n",
        "\n",
        "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval='step')\n",
        "early_stop_callback = pl.callbacks.EarlyStopping(monitor=\"val_loss\", patience=hparams[\"patience\"], mode='min')\n",
        "\n",
        "try:\n",
        "    trainer = pl.Trainer(gpus=1,\n",
        "                        precision=16,\n",
        "                        progress_bar_refresh_rate=1,\n",
        "                        max_epochs=hparams['max_epochs'],\n",
        "                        accumulate_grad_batches=hparams[\"accum_grads\"],\n",
        "                        check_val_every_n_epoch=1,\n",
        "                        callbacks=[early_stop_callback, checkpoint_callback, lr_monitor],\n",
        "                        checkpoint_callback=True, # Disable checkpoint saving.\n",
        "                        logger=neptune_logger,\n",
        "                        log_every_n_steps=20\n",
        "        )\n",
        "\n",
        "    trainer.fit(model, train_loader, valid_loader)\n",
        "    trainer.test(dataloaders=test_loader)\n",
        "except Exception as e:\n",
        "    print(\"Error:\", e)\n",
        "finally:\n",
        "    neptune_logger._run_instance.stop()"
      ],
      "metadata": {
        "id": "iUZp6QACKq4H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Experimentos-Troca dos pesos do Transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}