{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experimentos-Troca de pesos apenas uma camada.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experimentos trocando pesos de apenas uma camada\n",
        "\n",
        "- Esse notebook é destinado à experimentos rápidos de avaliação da performance do modelo baseline pré-treinado e fine-tuned trocando apenas os pesos de uma cabeça do Transformer;\n",
        "- O modelo baseline é o BERT base (bert-base-uncased) fine-tuned no dataset do IMDB de classificação de sentimentos;\n",
        "- Para troca de pesos, utiliza-se os pesos do Wav2Vec2 e inicializações randômicas (Uniforme, Normal e Xavier), dessa forma podemos verificar a degradação de performance do modelo, e de certa forma, verificar se os pesos do Wav2Vec2 irão trazer algum benefício, que no caso seria uma mínima degradação na performance se comparado com outros tipos de inicialização;"
      ],
      "metadata": {
        "id": "JCpco-HEVRql"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7yYzr6VeKrj"
      },
      "source": [
        "---\n",
        "## Bibliotecas e Instalações necessárias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdZuqxcnak9D"
      },
      "source": [
        "! pip -q install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lzpePIuwBEg"
      },
      "source": [
        "import os\n",
        "import copy\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Wav2Vec2ForCTC\n",
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSkfaTmGwDWd"
      },
      "source": [
        "def reset_seed():\n",
        "    random.seed(123)\n",
        "    np.random.seed(123)\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "reset_seed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xikuIXszwN4j"
      },
      "source": [
        "---\n",
        "## Preparando o dataset do IMDB"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download do dataset"
      ],
      "metadata": {
        "id": "-lckxKNEoWu6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8fQVdGgwG7u"
      },
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY6jD99twLfu"
      },
      "source": [
        "max_valid = 5000\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/valid.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "x_valid = x_train[-max_valid:]\n",
        "y_valid = y_train[-max_valid:]\n",
        "x_train = x_train[:-max_valid]\n",
        "y_train = y_train[:-max_valid]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_valid), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras validação:')\n",
        "for x, y in zip(x_valid[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras validação:')\n",
        "for x, y in zip(x_valid[-3:], y_valid[-3:]):\n",
        "    print(y, x[:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSfUsdGIwjHA"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cvxfI0UwmPy"
      },
      "source": [
        "class IMDBDataset():\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.x[idx], int(self.y[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(x, y, tokenizer, batch_size, shuffle=False, max_length=250):\n",
        "  def data_collator(batch):\n",
        "    x, y = zip(*batch)\n",
        "    tokenized_x = tokenizer(x, padding='longest', truncation=True, max_length=max_length, return_tensors='pt')\n",
        "    return tokenized_x['input_ids'], tokenized_x['attention_mask'], torch.LongTensor(y)\n",
        "  dataset = IMDBDataset(x, y)\n",
        "  return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=data_collator)"
      ],
      "metadata": {
        "id": "wLOspV_kZ15K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbxsHJK7xB1a"
      },
      "source": [
        "def test_model(model, test_loader):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        for tokens, mask, label in tqdm(test_loader):\n",
        "            tokens = tokens.to(device)\n",
        "            mask = mask.to(device)\n",
        "            label = label.to(device)\n",
        "        \n",
        "            pred = model(tokens, mask)['logits']\n",
        "\n",
        "            prediction = pred.argmax(dim=1)\n",
        "\n",
        "            acc += (prediction == label).sum() # ACC\n",
        "\n",
        "        test_acc = acc / len(test_loader.dataset)\n",
        "\n",
        "        print(\"ACC: \", test_acc.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF1Rx2LuxY1f"
      },
      "source": [
        "---\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using {device}\")"
      ],
      "metadata": {
        "id": "f4IhhcZZYw9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ecZMnOByOYG"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju2ag932sS7q"
      },
      "source": [
        "## Load Wav2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEbupV28bJQj"
      },
      "source": [
        " wav2vec2 = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_ED4SxRsqSI"
      },
      "source": [
        "## Load BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizando os pesos do BERT pré-treinado ('bert-base-uncased') fine-tuned no dataset do IMDB:"
      ],
      "metadata": {
        "id": "Pti2Px5s6lid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown"
      ],
      "metadata": {
        "id": "nUD9qukq6V0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://drive.google.com/u/0/uc?id=1jTI9u2nvnHXx2AYNLFjgEG1j4a8YXn9Z\"\n",
        "\n",
        "gdown.download(url, 'IMDBBert_finetuned.pt', quiet=False)"
      ],
      "metadata": {
        "id": "vXmJj1SE54va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizando o pesos do BERT já fine-tuned da Aula 6:"
      ],
      "metadata": {
        "id": "kahU2Owu4tyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states = torch.load('./IMDBBert_finetuned.pt')\n",
        "bert_imdb.load_state_dict(states)"
      ],
      "metadata": {
        "id": "HIRsE9Uw2GcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSkTfVwrzhWl"
      },
      "source": [
        "# IMDB: Baseline: Evaluate original fine-tuned BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzBvUn2hz_fA"
      },
      "source": [
        "test_loader = create_dataloader(x_test, y_test, tokenizer, hparams['bs'], shuffle=False, max_length=hparams['max_length'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuEz_-tHzutQ"
      },
      "source": [
        "test_model(bert_imdb, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_VbLsYe2dYC"
      },
      "source": [
        "# IMDB: Teste1: Evaluate original fine-tuned BERT with a layer from Wav2Vec2\n",
        "- Teste sugerido pelo Rodrigo durante a primeira apresentação do projeto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVBmpz8w3Ltz"
      },
      "source": [
        "bert_changed = copy.deepcopy(bert_imdb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD7N6-qC3Ufs"
      },
      "source": [
        "Qual o layer do Wav2Vec que vamos utilizar?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfpOFH-UZtlJ"
      },
      "source": [
        "# Mapping dos nomes só para alterarmos no carregamento dos pesos\n",
        "MAP_WAV2VEC_TO_BERT_NAMES = {\n",
        "    'attention.k_proj.weight': 'attention.self.key.weight',\n",
        "    'attention.k_proj.bias': 'attention.self.key.bias',\n",
        "    'attention.v_proj.weight': 'attention.self.value.weight',\n",
        "    'attention.v_proj.bias': 'attention.self.value.bias',\n",
        "    'attention.q_proj.weight': 'attention.self.query.weight',\n",
        "    'attention.q_proj.bias': 'attention.self.query.bias',\n",
        "    'attention.out_proj.weight': 'attention.output.dense.weight',\n",
        "    'attention.out_proj.bias': 'attention.output.dense.bias',\n",
        "    'layer_norm.weight': 'attention.output.LayerNorm.weight',\n",
        "    'layer_norm.bias': 'attention.output.LayerNorm.bias',\n",
        "    'feed_forward.intermediate_dense.weight': 'intermediate.dense.weight',\n",
        "    'feed_forward.intermediate_dense.bias': 'intermediate.dense.bias',\n",
        "    'feed_forward.output_dense.weight': 'output.dense.weight',\n",
        "    'feed_forward.output_dense.bias': 'output.dense.bias',\n",
        "    'final_layer_norm.weight': 'output.LayerNorm.weight',\n",
        "    'final_layer_norm.bias': 'output.LayerNorm.bias',\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8b9-0f72lTZ"
      },
      "source": [
        "wav2vec_11_attention_layer = wav2vec2.wav2vec2.encoder.layers[11]\n",
        "w2v_layer_states = wav2vec_11_attention_layer.state_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0UBhyG8ZngR"
      },
      "source": [
        "for k, v in w2v_layer_states.items():\n",
        "    bert_layer_state = 'bert.encoder.layer.11.' + MAP_WAV2VEC_TO_BERT_NAMES[k]\n",
        "    states[bert_layer_state] = v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5zpJLnehKTU"
      },
      "source": [
        "# bert_changed.bert.encoder.layer[11].output.LayerNorm = torch.nn.LayerNorm((768,), eps=1e-5, elementwise_affine=True)\n",
        "# bert_changed.bert.encoder.layer[11].attention.output.LayerNorm = torch.nn.LayerNorm((768,), eps=1e-5, elementwise_affine=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFpqLLlh2nDI"
      },
      "source": [
        "bert_changed.load_state_dict(states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYx8RP4L3Azr"
      },
      "source": [
        "test_model(bert_changed, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVDQ_JtSccfg"
      },
      "source": [
        "# IMDB: Teste 2: Evaluate original fine-tuned BERT with same layer initialized with RANDN (Normal Distribution)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMMAiLZacgrs"
      },
      "source": [
        "bert_changed = copy.deepcopy(bert_imdb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1fff7arciBc"
      },
      "source": [
        "for k, v in w2v_layer_states.items():\n",
        "    bert_layer_state = 'bert.encoder.layer.11.' + MAP_WAV2VEC_TO_BERT_NAMES[k]\n",
        "    states[bert_layer_state] = torch.randn_like(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUaBpC4Ucr_U"
      },
      "source": [
        "bert_changed.load_state_dict(states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgpKLqJ3cuFE"
      },
      "source": [
        "test_model(bert_changed, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmq4fPXXeDec"
      },
      "source": [
        "# IMDB: Teste 3: Evaluate original fine-tuned BERT with same layer initialized with RAND (Uniform Distribution)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nEmQE7UeFoR"
      },
      "source": [
        "bert_changed = copy.deepcopy(bert_imdb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9WEwnUCeH3E"
      },
      "source": [
        "for k, v in w2v_layer_states.items():\n",
        "    bert_layer_state = 'bert.encoder.layer.11.' + MAP_WAV2VEC_TO_BERT_NAMES[k]\n",
        "    states[bert_layer_state] = torch.rand_like(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omZQDS92eJek"
      },
      "source": [
        "bert_changed.load_state_dict(states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFC1qd4deKdE"
      },
      "source": [
        "test_model(bert_changed, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXWyP859joOD"
      },
      "source": [
        "# IMDB: Teste 4: Evaluate original fine-tuned BERT with same layer initialized using Xavier Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhaLXhRXj-6H"
      },
      "source": [
        "bert_changed = copy.deepcopy(bert_imdb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wOK-PcFjyxQ"
      },
      "source": [
        "for i, child in enumerate(bert_changed.bert.encoder.layer.children()):\n",
        "  if i == 11:\n",
        "      print(f\"Initializing head {i}\")\n",
        "      # SELF ATTENTION\n",
        "      torch.nn.init.xavier_uniform_(child.attention.self.query.weight)\n",
        "      child.attention.self.query.bias.data.fill_(0.01)\n",
        "\n",
        "      torch.nn.init.xavier_uniform_(child.attention.self.key.weight)\n",
        "      child.attention.self.key.bias.data.fill_(0.01)\n",
        "\n",
        "      torch.nn.init.xavier_uniform_(child.attention.self.value.weight)\n",
        "      child.attention.self.value.bias.data.fill_(0.01)\n",
        "\n",
        "      # ATTENTION - OUT\n",
        "      torch.nn.init.xavier_uniform_(child.attention.output.dense.weight)\n",
        "      child.attention.output.dense.bias.data.fill_(0.01)\n",
        "\n",
        "      child.attention.output.LayerNorm.reset_parameters()\n",
        "\n",
        "      torch.nn.init.xavier_uniform_(child.intermediate.dense.weight)\n",
        "      child.intermediate.dense.bias.data.fill_(0.01)\n",
        "      \n",
        "      torch.nn.init.xavier_uniform_(child.output.dense.weight)\n",
        "      child.output.dense.bias.data.fill_(0.01)\n",
        "\n",
        "      child.output.LayerNorm.reset_parameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irpkrKI5kM2v"
      },
      "source": [
        "test_model(bert_changed, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}